{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Search 2018 - Phase 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"head11\"></a> Code Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import tokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import pandas as pd\n",
    "from skimage.feature import hog\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.io import imread\n",
    "from sklearn.preprocessing import normalize\n",
    "from skimage import color\n",
    "from skimage import data, exposure\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import random\n",
    "from numpy.random import shuffle\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from skimage import img_as_ubyte\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "from keras.preprocessing import image\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(os.getcwd(), \"changing to:\", os.getcwd()+\"/../\")\n",
    "\n",
    "# Change this according to the path where you have the ws_toolkit\n",
    "ws_toolkit_path = os.getcwd()+\"/..\"\n",
    "\n",
    "os.chdir(ws_toolkit_path)\n",
    "print(os.getcwd())\n",
    "from ws_toolkit.utils import center_crop_image, k_neighbours, hoc, init_bow, process_images_keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cached arrays\n",
    "croppedImages = []\n",
    "X_HOC = None\n",
    "X_HOG = None\n",
    "X_BOW = None\n",
    "X_CNN = None\n",
    "CNN_PREDICTIONS = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cropImageList(images):\n",
    "    global croppedImages\n",
    "    if len(croppedImages) <= 0:\n",
    "        for imgName in images:\n",
    "            croppedImg = []\n",
    "            # Read image\n",
    "            img = imread(\"./images/\"+imgName)\n",
    "            # Resize image\n",
    "            croppedImg = center_crop_image(img, size=224)\n",
    "            croppedImages.append(croppedImg)\n",
    "    else:\n",
    "        print(\"Using cached cropped images\")\n",
    "    return croppedImages\n",
    "\n",
    "def features_hog(images, pixels_per_cell=(32, 32), orientations=8):\n",
    "    global X_HOG\n",
    "    if X_HOG is None:\n",
    "        X_HOG = []\n",
    "        for img in images:\n",
    "            # Convert to grayscale\n",
    "            img_gray = rgb2gray(img)   \n",
    "            # Extract HoG features\n",
    "            hist = hog(img_gray, orientations=orientations, pixels_per_cell=pixels_per_cell)   \n",
    "            # Normalize features\n",
    "            # We add 1 dimension to comply with scikit-learn API\n",
    "            hist = np.squeeze(normalize(hist.reshape(1, -1), norm=\"l2\"))  \n",
    "            X_HOG.append(hist)\n",
    "        # Creating a feature matrix for all images\n",
    "        X_HOG = np.array(X_HOG)\n",
    "    return X_HOG\n",
    "\n",
    "def getColorMatrix(_croppedImages, _bins=(4,4,4), _hsv=True): \n",
    "    global X_HOC\n",
    "    #Histogram of colors results\n",
    "    if X_HOC is None:   \n",
    "        X_HOC = []\n",
    "        for img in _croppedImages:    \n",
    "            # Change image color space from RGB to HSV. \n",
    "            # HSV color space was designed to more closely align with the way human vision perceive color-making attributes\n",
    "            img_q = img\n",
    "            if _hsv:\n",
    "                img_q = color.rgb2hsv(img)    \n",
    "            # convert image pixels to [0, 255] range, and to uint8 type\n",
    "            img_q = img_as_ubyte(img_q)\n",
    "            # Extract HoC features\n",
    "            hist, bin_edges = hoc(img_q, bins=_bins)    \n",
    "            # Normalize features\n",
    "            # We add 1 dimension to comply with scikit-learn API\n",
    "            hist = np.squeeze(normalize(hist.reshape(1, -1), norm=\"l2\"))    \n",
    "            X_HOC.append(hist)    \n",
    "        # Creating a feature matrix for all images\n",
    "        X_HOC = np.array(X_HOC)\n",
    "    return X_HOC\n",
    "\n",
    "def execute_bow(_dataTexts, _lemmatize=False, _mdf=3, _metric=\"cosine\", _k=10, _handles=False, _hashes=False, _case=False, _url=False):\n",
    "    global X_BOW\n",
    "    if X_BOW is None:\n",
    "        tknzr = tokenizer.TweetTokenizer(preserve_handles=_handles, preserve_hashes=_hashes, preserve_case=_case, preserve_url=_url)\n",
    "        vectorizer, X_BOW = init_bow(_dataTexts, {\"tknzr\": tknzr, \"lemmatize\": _lemmatize}, _mdf)\n",
    "    return X_BOW\n",
    "\n",
    "def getTagsBow(_dataImages):\n",
    "    global X_CNN\n",
    "    global CNN_PREDICTIONS\n",
    "    model = VGG16(weights='imagenet', include_top=True)\n",
    "    start = time.time()\n",
    "    if len(CNN_PREDICTIONS) <= 0:\n",
    "        start_i = time.time()\n",
    "        img_list = process_images_keras(_dataImages)\n",
    "        end_i = time.time()\n",
    "        print(\"Processed Images finished: {}\".format(end_i - start_i))\n",
    "        #model = ResNet50(weights='imagenet')\n",
    "        # Convert from list to ndarray\n",
    "        img_array_list = np.vstack(img_list)\n",
    "        # Feed all images to the model\n",
    "        print(\"No Cached Predictions\")\n",
    "        CNN_PREDICTIONS = model.predict(img_array_list)\n",
    "    else:\n",
    "        print(\"Using Cached Predictions\")\n",
    "    end = time.time()\n",
    "    print(\"Model Predictions finished: {}\".format(end - start))\n",
    "    #print(\"Resulting shape of the network output: {}\".format(preds.shape))\n",
    "    concepts = decode_predictions(CNN_PREDICTIONS, top=5)\n",
    "    # Experiment with this parameter\n",
    "    k = 5\n",
    "    # Get the top K most probable concepts per image\n",
    "    sorted_concepts =  np.argsort(CNN_PREDICTIONS, axis=1)[:,::-1][:,:k]\n",
    "    data_tags = concepts\n",
    "    mlb = MultiLabelBinarizer(classes=range(0,1000))\n",
    "    X_CNN = mlb.fit_transform(sorted_concepts)\n",
    "    #print(tags_bow.shape)\n",
    "    return X_CNN\n",
    "\n",
    "def computeFeatures():\n",
    "    pixels_per_cell = (32, 32)\n",
    "    orientations = 8\n",
    "    \n",
    "    bins = (4,4,4)\n",
    "    hsv = True\n",
    "    \n",
    "    _lemmatize=False\n",
    "    _mdf=3\n",
    "    _metric=\"cosine\"\n",
    "    _k=10\n",
    "    _handles=False\n",
    "    _hashes=False\n",
    "    _case=False\n",
    "    _url=False\n",
    "    \n",
    "    return features_hog(croppedImages, pixels_per_cell, orientations), getColorMatrix(croppedImages, bins, hsv), execute_bow(tweets, _lemmatize, _mdf, _metric, _k, _handles, _hashes, _case, _url), getTagsBow(imageLinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset .csv\n",
    "df = pd.read_csv(\"./visualstories_edfest_2016_twitter_xmedia.csv\",\n",
    "                 sep=';', encoding=\"utf-8\")\n",
    "\n",
    "data = np.array([df.get(\"text\").values, df.get(\n",
    "    \"image-url\").values, df.get(\"gt_class\").values])\n",
    "# This are the text of the tweets\n",
    "tweets = data[0]\n",
    "# This are the links of the images of the tweets (ex: C0zsADasd213.jpg)\n",
    "imageLinks = [i.replace('https://pbs.twimg.com/media/', '') for i in data[1]]\n",
    "# This are the arrays of the data of each cropped image\n",
    "targets = [list(map(int, c.replace(' ', '').split(\",\"))) for c in data[2]]\n",
    "# Save cropped images in cache\n",
    "croppedImages = cropImageList(imageLinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Cached Features Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache features\n",
    "X_HOG, X_HOC, X_BOW, X_CNN = computeFeatures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Label Propagation Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAlg(mlb, images, y, y_true, features, weights, selection, topk, threshold, alpha, iterations, params=None, mf = False, indices_unlabeled=[]):\n",
    "\n",
    "    # Step 2 - Normalize Y and Initialize matrix F with Y\n",
    "    Y_hidden = normalize(y, axis = 1, norm=\"l1\")\n",
    "    F = Y_hidden\n",
    "    #print(F[indices_labeled[0],:])\n",
    "    \n",
    "    # Step 3 - Compute matrix W (multi feature -mf true; or single feature -mf false)\n",
    "    W = []\n",
    "    if mf is True:\n",
    "        M = None\n",
    "        for i in range(len(features)):\n",
    "            M = weights[i]*euclidean_distances(features[i], features[i])\n",
    "        #M = weights[0]*euclidean_distances(feature[0], feature[0]) + weights[1]*euclidean_distances(feature[1], feature[1])\n",
    "    else:\n",
    "        M = euclidean_distances(features[0], features[0])\n",
    "        \n",
    "    sigma = np.std(M)\n",
    "    W = np.exp(-1 * M / (2*sigma**2))\n",
    "\n",
    "    # Step 4 - Normalize W\n",
    "    D = np.zeros(W.shape)\n",
    "    np.fill_diagonal(D, W.sum(axis=0))\n",
    "\n",
    "    D12 = np.zeros(D.shape)\n",
    "    from numpy.linalg import inv\n",
    "    D12 = inv(np.sqrt(D))\n",
    "\n",
    "    S = np.dot(D12, W)\n",
    "    S = np.dot(S, D12)\n",
    "\n",
    "    # Step 5 - Perform the F update step num_iterations steps\n",
    "    for i in range(1, iterations):\n",
    "        T1 = alpha * S\n",
    "        T1 = np.dot(T1, F)\n",
    "        T2 =  (1 - alpha) * Y_hidden\n",
    "        F = T1 + T2\n",
    "        #Normalizar para F (verficar segmentos)\n",
    "        F = normalize(F, axis = 1, norm=\"l1\")\n",
    "        \n",
    "    print(\"Indice unlabeled: {}\\nNormalized F: {}\".format(indices_unlabeled[0], F[indices_unlabeled[0],:]))\n",
    "    # Select top k classes\n",
    "    if selection is True:\n",
    "        F = np.fliplr(np.argsort(F, axis=1))\n",
    "        F = F[:,:topk]\n",
    "        Y = mlb.transform(F)\n",
    "    else:\n",
    "        T = []\n",
    "        for row in F:\n",
    "            T.append([i for i, v in enumerate(row) if v >= threshold])\n",
    "        Y = mlb.transform(T)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAll(iterations, p, features, weights, alpha, selection, topk, threshold, mf):\n",
    "    #Choose a random number between 1 and 100 to shuffle to prevent biased results\n",
    "    rand_seed = random.randint(1,100)\n",
    "    indices = np.arange(len(tweets))\n",
    "    np.random.seed(rand_seed)\n",
    "    shuffle(indices)\n",
    "\n",
    "    X = tweets\n",
    "    np.random.seed(rand_seed)\n",
    "    shuffle(X)\n",
    "\n",
    "    y_target = targets\n",
    "    np.random.seed(rand_seed)\n",
    "    shuffle(y_target)\n",
    "\n",
    "    total_images = X.shape[0]\n",
    "\n",
    "    # Let's assume that 20% of the dataset is labeled\n",
    "    labeled_set_size = int(total_images*p)\n",
    "\n",
    "    indices_labeled = indices[:labeled_set_size]\n",
    "    indices_unlabeled = indices[labeled_set_size:]\n",
    "    \n",
    "    print(\" \")\n",
    "    print(\"Iteration: {} - Total tweets labeled: {} - Total tweets unlabeled: {}\".format(iterations,\n",
    "        len(indices_labeled), len(indices_unlabeled)))\n",
    "\n",
    "    # Convert labels to a one-hot-encoded vector\n",
    "    # Keep groundtruth labels\n",
    "    classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "    # print(classes)\n",
    "    mlb = MultiLabelBinarizer(classes=classes)\n",
    "    #print(y_target[:1])\n",
    "    Y_true = mlb.fit_transform(y_target)\n",
    "    Y = mlb.transform(y_target)\n",
    "    #print(Y[:1])\n",
    "    # Remove labels of \"unlabeled\" data \n",
    "    Y[indices_unlabeled, :] = np.zeros(Y.shape[1])\n",
    "    \n",
    "    # Run Algorithm and Get Results\n",
    "    Y = runAlg(mlb, croppedImages, Y, Y_true, features=features, weights=weights, selection=selection, topk=topk, threshold=threshold,\n",
    "            alpha=alpha, iterations=iterations, mf=mf, indices_unlabeled=indices_unlabeled)\n",
    "\n",
    "    Y_pred = Y[indices_unlabeled, :]\n",
    "    y_gt = Y_true[indices_unlabeled, :]\n",
    "   \n",
    "    print(\"Ground Truth: {}\".format(Y_true[indices_unlabeled[0],:]))\n",
    "    print(\"Predicted:    {}\".format(Y[indices_unlabeled[0],:]))\n",
    "    \n",
    "    return Y_pred, y_gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "Evaluate the results of each run of the Iterative LP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.zeros((10,4))\n",
    "range_iterations = range(10,500,50)\n",
    "j = 0\n",
    "\n",
    "# Variable params\n",
    "p=0.7\n",
    "features=[X_CNN] # Possible X_HOG, X_HOG, X_BOW, X_CNN -> If mf=False use only one feature inside the array\n",
    "weights=[0.4, 0.6] # Sum must be one - and must always be filled even if mf=False\n",
    "alpha=0.2\n",
    "selection=False # False - Top K | True - Threshold\n",
    "topk=3\n",
    "threshold=0.05\n",
    "mf=True\n",
    "\n",
    "for i in range_iterations:\n",
    "    Y_pred, y_gt = runAll(i, p, features, weights, alpha, selection, topk, threshold, mf)\n",
    "    precision, recall, fscore, support = score(y_gt, Y_pred, average='macro')\n",
    "    results[j] = [precision, recall, fscore, support]\n",
    "    j = j+1\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "print(\"\\nResults List:    \\n{}\".format(results))\n",
    "print(\"\\nResults Report:  \\n{}\".format(classification_report(y_gt, Y_pred)))\n",
    "\n",
    "print(\"\\nResults Graph:   \\n\")\n",
    "\n",
    "colors = ['r', 'b', 'g']\n",
    "labels = ['precision', 'recall', 'f-score']\n",
    "\n",
    "for i in range(3):\n",
    "    plt.plot(range_iterations, results[:,i], colors[i], label=labels[i])\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "#plt.plot(range_iterations, results[:,0], 'r', range_iterations, results[:,1], 'b', range_iterations, results[:,2], 'g')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
