{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Search 2018 - Tutorial 5: Multi-Feature Label Propagation\n",
    "## Contents\n",
    "\n",
    "1. [Overview](#head1)\n",
    "  1. [Code Imports](#head11)\n",
    "2. [Iterative Label Propagation on Web Data](#head2)\n",
    "  1. [Multi-label LP Algorithm](#head21)\n",
    "  2. [Implement the Multi-label LP Algorithm](#head22)\n",
    "  3. [Evaluation](#head23)\n",
    "  4. [Exercises](#head24)\n",
    "3. [Multi-Feature Iterative Label Propagation](#head3)\n",
    "  1. [Implement the Multi-Feature Iterative Label Propagation](#head31)\n",
    "  2. [Exercises](#head32)\n",
    "\n",
    "## <a name=\"head1\"></a> Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lab you implemented the Iterative Label Propagation algorithm, which consists of a semi-supervised graph approach to annotate uncategorized/unlabelled data starting from a small set of categorized/labelled data.\n",
    "\n",
    "The target dataset was the MNIST Digits, which is only adequate for implementation purposes (i.e. testing, debugging, etc.). In this lab, the first step will be to apply the LP algorithm to Web data and analyse its behaviour.\n",
    "\n",
    "\n",
    "Additionally, in the LP implementation of the previous lab, semantic affinity between documents (images/texts) was computed based on a **single feature space**. In this lab the LP algorithm definition will be revisited in order to accomodate the computation of semantic affinity between documents under **multiple feature spaces**. This will allow the construction of a much more richer graph, supporting propagation of labels by different similarity criteria. \n",
    "\n",
    "\n",
    "**Lab objectives:**\n",
    "* Apply the iterative version of the Label Propagation algorithm to Web data scenario and analyse the results;\n",
    "* Implement the Multi-feature iterative Label Propagation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"head11\"></a> Code Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/franciscorodrigues/Projects/PW/pw_phase2/labs changing to: /Users/franciscorodrigues/Projects/PW/pw_phase2/labs/../\n",
      "/Users/franciscorodrigues/Projects/PW/pw_phase2\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/franciscorodrigues/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from tokenizer import tokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import pandas as pd\n",
    "from skimage.feature import hog\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.io import imread\n",
    "from sklearn.preprocessing import normalize\n",
    "from skimage import color\n",
    "from skimage import data, exposure\n",
    "import random\n",
    "from numpy.random import shuffle\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from skimage import img_as_ubyte\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "from keras.preprocessing import image\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(os.getcwd(), \"changing to:\", os.getcwd()+\"/../\")\n",
    "\n",
    "# Change this according to the path where you have the ws_toolkit\n",
    "ws_toolkit_path = os.getcwd()+\"/..\"\n",
    "\n",
    "os.chdir(ws_toolkit_path)\n",
    "print(os.getcwd())\n",
    "from ws_toolkit.utils import center_crop_image, k_neighbours, hoc, init_bow, process_images_keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cached arrays\n",
    "colorMatrix = None\n",
    "gradientMatrix = None\n",
    "croppedImages = []\n",
    "dataSetPredictions = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cropImageList(images):\n",
    "    global croppedImages\n",
    "    if len(croppedImages) <= 0:\n",
    "        for imgName in images:\n",
    "            croppedImg = []\n",
    "            # Read image\n",
    "            img = imread(\"./images/\"+imgName)\n",
    "            # Resize image\n",
    "            croppedImg = center_crop_image(img, size=224)\n",
    "            croppedImages.append(croppedImg)\n",
    "    else:\n",
    "        print(\"Using cached cropped images\")\n",
    "    return croppedImages\n",
    "\n",
    "# Read dataset .csv\n",
    "df = pd.read_csv(\"./visualstories_edfest_2016_twitter_xmedia.csv\",\n",
    "                 sep=';', encoding=\"utf-8\")\n",
    "\n",
    "data = np.array([df.get(\"text\").values, df.get(\n",
    "    \"image-url\").values, df.get(\"gt_class\").values])\n",
    "# This are the text of the tweets\n",
    "tweets = data[0]\n",
    "# This are the links of the images of the tweets (ex: C0zsADasd213.jpg)\n",
    "imageLinks = [i.replace('https://pbs.twimg.com/media/', '') for i in data[1]]\n",
    "# This are the arrays of the data of each cropped image\n",
    "targets = [list(map(int, c.replace(' ', '').split(\",\"))) for c in data[2]]\n",
    "# Save cropped images in cache\n",
    "croppedImages = cropImageList(imageLinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_hog(images, pixels_per_cell=(32, 32), orientations=8):\n",
    "    global gradientMatrix\n",
    "    if gradientMatrix is None:\n",
    "        gradientMatrix = []\n",
    "        for img in images:\n",
    "            # Convert to grayscale\n",
    "            img_gray = rgb2gray(img)   \n",
    "            # Extract HoG features\n",
    "            hist = hog(img_gray, orientations=orientations, pixels_per_cell=pixels_per_cell)   \n",
    "            # Normalize features\n",
    "            # We add 1 dimension to comply with scikit-learn API\n",
    "            hist = np.squeeze(normalize(hist.reshape(1, -1), norm=\"l2\"))  \n",
    "            gradientMatrix.append(hist)\n",
    "        # Creating a feature matrix for all images\n",
    "        gradientMatrix = np.array(gradientMatrix)\n",
    "    return gradientMatrix\n",
    "\n",
    "def getColorMatrix(_croppedImages, _bins=(4,4,4), _hsv=True): \n",
    "    global colorMatrix\n",
    "    #Histogram of colors results\n",
    "    if colorMatrix is None:   \n",
    "        colorMatrix = []\n",
    "        for img in _croppedImages:    \n",
    "            # Change image color space from RGB to HSV. \n",
    "            # HSV color space was designed to more closely align with the way human vision perceive color-making attributes\n",
    "            img_q = img\n",
    "            if _hsv:\n",
    "                img_q = color.rgb2hsv(img)    \n",
    "            # convert image pixels to [0, 255] range, and to uint8 type\n",
    "            img_q = img_as_ubyte(img_q)\n",
    "            # Extract HoC features\n",
    "            hist, bin_edges = hoc(img_q, bins=_bins)    \n",
    "            # Normalize features\n",
    "            # We add 1 dimension to comply with scikit-learn API\n",
    "            hist = np.squeeze(normalize(hist.reshape(1, -1), norm=\"l2\"))    \n",
    "            colorMatrix.append(hist)    \n",
    "        # Creating a feature matrix for all images\n",
    "        colorMatrix = np.array(colorMatrix)\n",
    "    return colorMatrix\n",
    "\n",
    "def execute_bow(_dataTexts, _lemmatize=False, _mdf=3, _metric=\"cosine\", _k=10, _handles=False, _hashes=False, _case=False, _url=False):\n",
    "    tknzr = tokenizer.TweetTokenizer(preserve_handles=_handles, preserve_hashes=_hashes, preserve_case=_case, preserve_url=_url)\n",
    "    vectorizer, texts_bow = init_bow(_dataTexts, {\"tknzr\": tknzr, \"lemmatize\": _lemmatize}, _mdf)\n",
    "    return texts_bow\n",
    "\n",
    "def getTagsBow(_dataImages):\n",
    "    global dataSetPredictions\n",
    "    model = VGG16(weights='imagenet', include_top=True)\n",
    "    start = time.time()\n",
    "    if len(dataSetPredictions) <= 0:\n",
    "        start_i = time.time()\n",
    "        img_list = process_images_keras(_dataImages)\n",
    "        end_i = time.time()\n",
    "        print(\"Processed Images finished: {}\".format(end_i - start_i))\n",
    "        #model = ResNet50(weights='imagenet')\n",
    "        # Convert from list to ndarray\n",
    "        img_array_list = np.vstack(img_list)\n",
    "        # Feed all images to the model\n",
    "        print(\"No Cached Predictions\")\n",
    "        dataSetPredictions = model.predict(img_array_list)\n",
    "    else:\n",
    "        print(\"Using Cached Predictions\")\n",
    "    end = time.time()\n",
    "    print(\"Model Predictions finished: {}\".format(end - start))\n",
    "    #print(\"Resulting shape of the network output: {}\".format(preds.shape))\n",
    "    concepts = decode_predictions(dataSetPredictions, top=5)\n",
    "    # Experiment with this parameter\n",
    "    k = 5\n",
    "    # Get the top K most probable concepts per image\n",
    "    sorted_concepts =  np.argsort(dataSetPredictions, axis=1)[:,::-1][:,:k]\n",
    "    data_tags = concepts\n",
    "    mlb = MultiLabelBinarizer(classes=range(0,1000))\n",
    "    tags_bow = mlb.fit_transform(sorted_concepts)\n",
    "    #print(tags_bow.shape)\n",
    "    return tags_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"head2\"></a> Iterative Label Propagation on Web Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a dataset $X=\\{x_1, x_2, \\ldots, x_L, \\ \\ x_{L+1}, \\ldots, x_N\\}$, with $N$ data points, where each $x_i$ consists of some feature representation of document $i$. Given a categories set $C=\\{1, 2, \\ldots, |C|\\}$, it is assumed that the first $L$ data points are labelled with a label $c \\in C$, and the remaining ones are unlabelled.\n",
    "\n",
    "Please refer to the \"Mining Data Graphs\" class (lectured on 29/10), namely slides 28, 29 and 30, for a description of the algorithm steps.\n",
    "\n",
    "For more information, you can check the original paper: Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, Bernhard Schoelkopf. Learning with local and global consistency (2004) http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.115.3219\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"head21\"></a> Multi-label LP Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the Iterative LP algorithm on your project dataset and discuss its effectiveness.\n",
    "\n",
    "The dataset has a total of 13 categories. The categories of each document are available in the corresponding line of that document (column 'gt_class'), in the provided .csv file. Multiple categories are separated by a comma ','. You should represent each document's categories as a 13-dimensional vector (one-hot encoding), as you did for the MNIST dataset. In this case, you may have more than 1 active dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"head22\"></a> Implement the Multi-label LP Algorithm\n",
    "\n",
    "**Multi-label LP:** As each document has multiple categories, you will need to modify your LP implementation. Instead of an **argmax** to select the final category of each document, you will need to **select the top-k categories**, by applying a threshold on the number of categories assigned.\n",
    "\n",
    "**Discuss:** Discuss examples of thresholds (e.g. select the top-3 labels, keep all categories with their values $>\\alpha$, etc.)  .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAlg(mlb, images, y, y_true, feature, selection, topk, threshold, alpha, iterations, params=None, mf = False, indices_unlabeled=[]):\n",
    "   \n",
    "    # Step 1 - Extract features for each image (HoG/CNN/HoC) in X\n",
    "    pixels_per_cell = (32, 32)\n",
    "    orientations = 8\n",
    "    \n",
    "    bins = (4,4,4)\n",
    "    hsv = True\n",
    "    \n",
    "    _lemmatize=False\n",
    "    _mdf=3\n",
    "    _metric=\"cosine\"\n",
    "    _k=10\n",
    "    _handles=False\n",
    "    _hashes=False\n",
    "    _case=False\n",
    "    _url=False\n",
    "    \n",
    "    if mf is True:\n",
    "        X1 = features_hog(images, pixels_per_cell, orientations)\n",
    "        X2 = getColorMatrix(images, bins, hsv)\n",
    "        X3 = execute_bow(tweets, _lemmatize, _mdf, _metric, _k, _handles, _hashes, _case, _url)\n",
    "        X4 = getTagsBow(imageLinks)\n",
    "    else:\n",
    "        if feature is \"hog\":\n",
    "            X = features_hog(images, pixels_per_cell, orientations)\n",
    "        elif feature is \"hoc\":\n",
    "            X = getColorMatrix(images, bins, hsv)\n",
    "        elif feature is \"bow\":\n",
    "            X = execute_bow(tweets, _lemmatize, _mdf, _metric, _k, _handles, _hashes, _case, _url)\n",
    "        else:\n",
    "            X = getTagsBow(imageLinks)\n",
    "\n",
    "    # Step 2 - Normalize Y and Initialize matrix F with Y\n",
    "    Y_hidden = normalize(y, axis = 1, norm=\"l1\")\n",
    "    F = Y_hidden\n",
    "    #print(F[indices_labeled[0],:])\n",
    "    \n",
    "    # Step 3 - Compute matrix W (multi feature -mf true; or single feature -mf false)\n",
    "    W = []\n",
    "    if mf is True:\n",
    "        M = 0.1*euclidean_distances(X1, X1)+ 0.1*euclidean_distances(X2, X2) + 0.3*euclidean_distances(X3, X3) + 0.5*euclidean_distances(X4, X4)\n",
    "        sigma = np.std(M)\n",
    "        W = np.exp(-1 * M / (2*sigma**2))\n",
    "    else:\n",
    "        M = euclidean_distances(X, X)\n",
    "        sigma = np.std(M)\n",
    "        W = np.exp(-1 * M / (2*sigma**2))\n",
    "\n",
    "    # Step 4 - Normalize W\n",
    "    D = np.zeros(W.shape)\n",
    "    np.fill_diagonal(D, W.sum(axis=0))\n",
    "\n",
    "    D12 = np.zeros(D.shape)\n",
    "    from numpy.linalg import inv\n",
    "    D12 = inv(np.sqrt(D))\n",
    "\n",
    "    S = np.dot(D12, W)\n",
    "    S = np.dot(S, D12)\n",
    "\n",
    "    # Step 5 - Perform the F update step num_iterations steps\n",
    "    for i in range(1, iterations):\n",
    "        T1 = alpha * S\n",
    "        T1 = np.dot(T1, F)\n",
    "        T2 =  (1 - alpha) * Y_hidden\n",
    "        F = T1 + T2\n",
    "        #Normalizar para F (verficar segmentos)\n",
    "        F = normalize(F, axis = 1, norm=\"l1\")\n",
    "        \n",
    "    print(\"Indice unlabeled: {}\\nNormalized F: {}\".format(indices_unlabeled[0], F[indices_unlabeled[0],:]))\n",
    "    # Select top k classes\n",
    "    if selection is True:\n",
    "        F = np.fliplr(np.argsort(F, axis=1))\n",
    "        F = F[:,:topk]\n",
    "        Y = mlb.transform(F)\n",
    "    else:\n",
    "        T = []\n",
    "        for row in F:\n",
    "            T.append([i for i, v in enumerate(row) if v >= threshold])\n",
    "        Y = mlb.transform(T)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAll(iterations, p, feature, alpha, selection, topk, threshold, mf):\n",
    "    #Choose a random number between 1 and 100 to shuffle to prevent biased results\n",
    "    rand_seed = random.randint(1,100)\n",
    "    indices = np.arange(len(tweets))\n",
    "    np.random.seed(rand_seed)\n",
    "    shuffle(indices)\n",
    "\n",
    "    X = tweets\n",
    "    np.random.seed(rand_seed)\n",
    "    shuffle(X)\n",
    "\n",
    "    y_target = targets\n",
    "    np.random.seed(rand_seed)\n",
    "    shuffle(y_target)\n",
    "\n",
    "    total_images = X.shape[0]\n",
    "\n",
    "    # Let's assume that 20% of the dataset is labeled\n",
    "    labeled_set_size = int(total_images*p)\n",
    "\n",
    "    indices_labeled = indices[:labeled_set_size]\n",
    "    indices_unlabeled = indices[labeled_set_size:]\n",
    "    \n",
    "    print(\" \")\n",
    "    print(\"Iteration: {} - Total tweets labeled: {} - Total tweets unlabeled: {}\".format(iterations,\n",
    "        len(indices_labeled), len(indices_unlabeled)))\n",
    "\n",
    "\n",
    "    # Convert labels to a one-hot-encoded vector\n",
    "    # Keep groundtruth labels\n",
    "    classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "    # print(classes)\n",
    "    mlb = MultiLabelBinarizer(classes=classes)\n",
    "    #print(y_target[:1])\n",
    "    Y_true = mlb.fit_transform(y_target)\n",
    "    Y = mlb.transform(y_target)\n",
    "    #print(Y[:1])\n",
    "    # Remove labels of \"unlabeled\" data \n",
    "    Y[indices_unlabeled, :] = np.zeros(Y.shape[1])\n",
    "    \n",
    "    # Run Algorithm and Get Results\n",
    "    Y = runAlg(mlb, croppedImages, Y, Y_true, feature=feature, selection=selection, topk=topk, threshold=threshold,\n",
    "            alpha=alpha, iterations=iterations, mf=mf, indices_unlabeled=indices_unlabeled)\n",
    "\n",
    "    Y_pred = Y[indices_unlabeled, :]\n",
    "    y_gt = Y_true[indices_unlabeled, :]\n",
    "   \n",
    "    \n",
    "    print(\"Ground Truth: {}\".format(Y_true[indices_unlabeled[0],:]))\n",
    "    print(\"Predicted:    {}\".format(Y[indices_unlabeled[0],:]))\n",
    "    \n",
    "   \n",
    "    return Y_pred, y_gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"head23\"></a>  Evaluation\n",
    "Evaluate the results of each run of the Iterative LP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Iteration: 10 - Total tweets labeled: 200 - Total tweets unlabeled: 1800\n",
      "Indice unlabeled: 522\n",
      "Normalized F: [0.07839401 0.02506253 0.01126631 0.00529452 0.18249851 0.0315571\n",
      " 0.03365594 0.00590294 0.28056799 0.0899039  0.10802464 0.05104667\n",
      " 0.09682496]\n",
      "Ground Truth: [0 0 0 0 1 0 0 0 1 0 0 0 1]\n",
      "Predicted:    [0 0 0 0 1 0 0 0 1 0 0 0 0]\n",
      " \n",
      "Iteration: 60 - Total tweets labeled: 200 - Total tweets unlabeled: 1800\n",
      "Indice unlabeled: 652\n",
      "Normalized F: [3.91570251e-03 8.49066717e-03 8.18270484e-04 9.00050614e-01\n",
      " 1.98547318e-02 1.40498490e-03 1.14546050e-02 6.15782192e-04\n",
      " 2.62717799e-02 9.75766877e-03 1.25005435e-02 2.23099638e-05\n",
      " 4.84234032e-03]\n",
      "Ground Truth: [0 0 0 0 0 0 0 0 0 1 1 1 0]\n",
      "Predicted:    [0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " \n",
      "Iteration: 110 - Total tweets labeled: 200 - Total tweets unlabeled: 1800\n",
      "Indice unlabeled: 1387\n",
      "Normalized F: [0.00911428 0.00822749 0.05658042 0.00725242 0.0142793  0.00985205\n",
      " 0.52072849 0.00547202 0.06356602 0.04171948 0.06197938 0.01846903\n",
      " 0.18275961]\n",
      "Ground Truth: [0 0 0 0 1 0 0 0 1 0 0 0 0]\n",
      "Predicted:    [0 0 0 0 0 0 1 0 0 0 0 0 1]\n",
      " \n",
      "Iteration: 160 - Total tweets labeled: 200 - Total tweets unlabeled: 1800\n",
      "Indice unlabeled: 640\n",
      "Normalized F: [0.05159479 0.00075972 0.01234447 0.02164571 0.12830587 0.03707199\n",
      " 0.07743667 0.05411067 0.2264823  0.02497482 0.14362613 0.0061812\n",
      " 0.21546567]\n",
      "Ground Truth: [1 1 0 1 0 0 0 0 1 1 0 0 1]\n",
      "Predicted:    [0 0 0 0 0 0 0 0 1 0 0 0 1]\n",
      " \n",
      "Iteration: 210 - Total tweets labeled: 200 - Total tweets unlabeled: 1800\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-496b3cedcf3b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange_iterations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mY_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_gt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunAll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_gt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msupport\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-59de78abceb1>\u001b[0m in \u001b[0;36mrunAll\u001b[0;34m(iterations, p, feature, alpha, selection, topk, threshold, mf)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;31m# Run Algorithm and Get Results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     Y = runAlg(mlb, croppedImages, Y, Y_true, feature=feature, selection=selection, topk=topk, threshold=threshold,\n\u001b[0;32m---> 43\u001b[0;31m             alpha=alpha, iterations=iterations, mf=mf, indices_unlabeled=indices_unlabeled)\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mY_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindices_unlabeled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-7e7f3f3b936d>\u001b[0m in \u001b[0;36mrunAlg\u001b[0;34m(mlb, images, y, y_true, feature, selection, topk, threshold, alpha, iterations, params, mf, indices_unlabeled)\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mD12\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mD12\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36minv\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0msignature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'D->D'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'd->d'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[0mextobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_linalg_error_extobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_raise_linalgerror_singular\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m     \u001b[0mainv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mainv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "\n",
    "results = np.zeros((10,4))\n",
    "range_iterations = range(10,500,50)\n",
    "j = 0\n",
    "\n",
    "# Variable params\n",
    "p=0.1\n",
    "feature=\"hog\"\n",
    "alpha=0.2\n",
    "selection=False # False - Top K | True - Threshold\n",
    "topk=3\n",
    "threshold=0.15\n",
    "mf=False\n",
    "\n",
    "for i in range_iterations:\n",
    "    Y_pred, y_gt = runAll(i, p, feature, alpha, selection, topk, threshold, mf)\n",
    "    precision, recall, fscore, support = score(y_gt, Y_pred, average='macro')\n",
    "    results[j] = [precision, recall, fscore, support]\n",
    "    j = j+1\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "print(\"\\nResults List:    \\n{}\".format(results))\n",
    "print(\"\\nResults Report:  \\n{}\".format(classification_report(y_gt, Y_pred)))\n",
    "\n",
    "print(\"\\nResults Graph:   \\n\")\n",
    "# %%\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range_iterations, results[:,0], 'r', range_iterations, results[:,1], 'b', range_iterations, results[:,2], 'g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"head24\"></a>  Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which feature spaces are more effective? Do a per-class inspection and understand which feature spaces are more effective for each class and why. \n",
    "\n",
    "# How does the LP algorithm behaves when you change the number of initial labels? (variable labeled_set_size on lab4)\n",
    "\n",
    "# Note that documents (Tweets) from your project's dataset have multiple labels, i.e. each document may belong to 1 or more classes. \n",
    "# Discuss how this impacts the label propagation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"head3\"></a> Multi-Feature Iterative Label Propagation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the computation of the affinity matrix S.\n",
    "\n",
    "Given some feature space representation, each entry $w_{ij}$, for $i\\neq j$ is computed as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w_{ij} = exp\\Big({-\\frac{||x_i - x_j||^2}{2\\sigma^2}}\\Big),\n",
    "\\end{align}\n",
    "$$\n",
    "where a Gaussian kernel is applied over the distance on the considered feature space.\n",
    "\n",
    "\n",
    "In order compute affinity by considering **multiple feature spaces**, the above expression can be extended as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w_{ij} = exp\\Big({-\\frac{\\Big[\\sum_{f}\\alpha_f\\cdot||x_i^f - x_j^f||\\Big]^2}{2\\sigma^2}}\\Big),\n",
    "\\end{align}\n",
    "$$\n",
    "where each $x^f$ denotes a given feature space and $\\alpha_f$ the weight associated with that space. The weights should be defined such that $\\sum_f \\alpha_f = 1$.\n",
    "\n",
    "You can define the contribution of each feature space by adequately setting its associated weight $\\alpha_f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <a name=\"head31\"></a> Implement the Multi-Feature Iterative Label Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that given the Multi-label implementation of Iterative LP, you should only need to change the computation of each $w_{ij}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"head32\"></a>  Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discuss the effectiveness of the Multi-Feature approach versus the Single feature variant. Namely, compare HoC+HoG with VGG.\n",
    "\n",
    "# Change the weights of each feature space and interpret the results. Which features better contribute to the overall effectiveness?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
