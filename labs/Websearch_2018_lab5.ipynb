{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Search 2018 - Tutorial 5: Multi-Feature Label Propagation\n",
    "## Contents\n",
    "\n",
    "1. [Overview](#head1)\n",
    "  1. [Code Imports](#head11)\n",
    "2. [Iterative Label Propagation on Web Data](#head2)\n",
    "  1. [Multi-label LP Algorithm](#head21)\n",
    "  2. [Implement the Multi-label LP Algorithm](#head22)\n",
    "  3. [Evaluation](#head23)\n",
    "  4. [Exercises](#head24)\n",
    "3. [Multi-Feature Iterative Label Propagation](#head3)\n",
    "  1. [Implement the Multi-Feature Iterative Label Propagation](#head31)\n",
    "  2. [Exercises](#head32)\n",
    "\n",
    "## <a name=\"head1\"></a> Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous lab you implemented the Iterative Label Propagation algorithm, which consists of a semi-supervised graph approach to annotate uncategorized/unlabelled data starting from a small set of categorized/labelled data.\n",
    "\n",
    "The target dataset was the MNIST Digits, which is only adequate for implementation purposes (i.e. testing, debugging, etc.). In this lab, the first step will be to apply the LP algorithm to Web data and analyse its behaviour.\n",
    "\n",
    "\n",
    "Additionally, in the LP implementation of the previous lab, semantic affinity between documents (images/texts) was computed based on a **single feature space**. In this lab the LP algorithm definition will be revisited in order to accomodate the computation of semantic affinity between documents under **multiple feature spaces**. This will allow the construction of a much more richer graph, supporting propagation of labels by different similarity criteria. \n",
    "\n",
    "\n",
    "**Lab objectives:**\n",
    "* Apply the iterative version of the Label Propagation algorithm to Web data scenario and analyse the results;\n",
    "* Implement the Multi-feature iterative Label Propagation algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"head11\"></a> Code Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import tokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import linalg as LA\n",
    "import pandas as pd\n",
    "from skimage.feature import hog\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.io import imread\n",
    "from sklearn.preprocessing import normalize\n",
    "from skimage import color\n",
    "from skimage import data, exposure\n",
    "import random\n",
    "from numpy.random import shuffle\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from skimage import img_as_ubyte\n",
    "import warnings\n",
    "import os\n",
    "import time\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "from keras.preprocessing import image\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(os.getcwd(), \"changing to:\", os.getcwd()+\"/../\")\n",
    "\n",
    "# Change this according to the path where you have the ws_toolkit\n",
    "ws_toolkit_path = os.getcwd()+\"/..\"\n",
    "\n",
    "os.chdir(ws_toolkit_path)\n",
    "print(os.getcwd())\n",
    "from ws_toolkit.utils import center_crop_image, k_neighbours, hoc, init_bow, process_images_keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cached arrays\n",
    "croppedImages = []\n",
    "X_HOC = None\n",
    "X_HOG = None\n",
    "X_BOW = None\n",
    "X_CNN = None\n",
    "CNN_PREDICTIONS = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cropImageList(images):\n",
    "    global croppedImages\n",
    "    if len(croppedImages) <= 0:\n",
    "        for imgName in images:\n",
    "            croppedImg = []\n",
    "            # Read image\n",
    "            img = imread(\"./images/\"+imgName)\n",
    "            # Resize image\n",
    "            croppedImg = center_crop_image(img, size=224)\n",
    "            croppedImages.append(croppedImg)\n",
    "    else:\n",
    "        print(\"Using cached cropped images\")\n",
    "    return croppedImages\n",
    "\n",
    "# Read dataset .csv\n",
    "df = pd.read_csv(\"./visualstories_edfest_2016_twitter_xmedia.csv\",\n",
    "                 sep=';', encoding=\"utf-8\")\n",
    "\n",
    "data = np.array([df.get(\"text\").values, df.get(\n",
    "    \"image-url\").values, df.get(\"gt_class\").values])\n",
    "# This are the text of the tweets\n",
    "tweets = data[0]\n",
    "# This are the links of the images of the tweets (ex: C0zsADasd213.jpg)\n",
    "imageLinks = [i.replace('https://pbs.twimg.com/media/', '') for i in data[1]]\n",
    "# This are the arrays of the data of each cropped image\n",
    "targets = [list(map(int, c.replace(' ', '').split(\",\"))) for c in data[2]]\n",
    "# Save cropped images in cache\n",
    "croppedImages = cropImageList(imageLinks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_hog(images, pixels_per_cell=(32, 32), orientations=8):\n",
    "    global X_HOG\n",
    "    if X_HOG is None:\n",
    "        X_HOG = []\n",
    "        for img in images:\n",
    "            # Convert to grayscale\n",
    "            img_gray = rgb2gray(img)   \n",
    "            # Extract HoG features\n",
    "            hist = hog(img_gray, orientations=orientations, pixels_per_cell=pixels_per_cell)   \n",
    "            # Normalize features\n",
    "            # We add 1 dimension to comply with scikit-learn API\n",
    "            hist = np.squeeze(normalize(hist.reshape(1, -1), norm=\"l2\"))  \n",
    "            X_HOG.append(hist)\n",
    "        # Creating a feature matrix for all images\n",
    "        X_HOG = np.array(X_HOG)\n",
    "    return X_HOG\n",
    "\n",
    "def getColorMatrix(_croppedImages, _bins=(4,4,4), _hsv=True): \n",
    "    global X_HOC\n",
    "    #Histogram of colors results\n",
    "    if X_HOC is None:   \n",
    "        X_HOC = []\n",
    "        for img in _croppedImages:    \n",
    "            # Change image color space from RGB to HSV. \n",
    "            # HSV color space was designed to more closely align with the way human vision perceive color-making attributes\n",
    "            img_q = img\n",
    "            if _hsv:\n",
    "                img_q = color.rgb2hsv(img)    \n",
    "            # convert image pixels to [0, 255] range, and to uint8 type\n",
    "            img_q = img_as_ubyte(img_q)\n",
    "            # Extract HoC features\n",
    "            hist, bin_edges = hoc(img_q, bins=_bins)    \n",
    "            # Normalize features\n",
    "            # We add 1 dimension to comply with scikit-learn API\n",
    "            hist = np.squeeze(normalize(hist.reshape(1, -1), norm=\"l2\"))    \n",
    "            X_HOC.append(hist)    \n",
    "        # Creating a feature matrix for all images\n",
    "        X_HOC = np.array(X_HOC)\n",
    "    return X_HOC\n",
    "\n",
    "def execute_bow(_dataTexts, _lemmatize=False, _mdf=3, _metric=\"cosine\", _k=10, _handles=False, _hashes=False, _case=False, _url=False):\n",
    "    global X_BOW\n",
    "    if X_BOW is None:\n",
    "        tknzr = tokenizer.TweetTokenizer(preserve_handles=_handles, preserve_hashes=_hashes, preserve_case=_case, preserve_url=_url)\n",
    "        vectorizer, X_BOW = init_bow(_dataTexts, {\"tknzr\": tknzr, \"lemmatize\": _lemmatize}, _mdf)\n",
    "    return X_BOW\n",
    "\n",
    "def getTagsBow(_dataImages):\n",
    "    global X_CNN\n",
    "    global CNN_PREDICTIONS\n",
    "    model = VGG16(weights='imagenet', include_top=True)\n",
    "    start = time.time()\n",
    "    if len(CNN_PREDICTIONS) <= 0:\n",
    "        start_i = time.time()\n",
    "        img_list = process_images_keras(_dataImages)\n",
    "        end_i = time.time()\n",
    "        print(\"Processed Images finished: {}\".format(end_i - start_i))\n",
    "        #model = ResNet50(weights='imagenet')\n",
    "        # Convert from list to ndarray\n",
    "        img_array_list = np.vstack(img_list)\n",
    "        # Feed all images to the model\n",
    "        print(\"No Cached Predictions\")\n",
    "        CNN_PREDICTIONS = model.predict(img_array_list)\n",
    "    else:\n",
    "        print(\"Using Cached Predictions\")\n",
    "    end = time.time()\n",
    "    print(\"Model Predictions finished: {}\".format(end - start))\n",
    "    #print(\"Resulting shape of the network output: {}\".format(preds.shape))\n",
    "    concepts = decode_predictions(CNN_PREDICTIONS, top=5)\n",
    "    # Experiment with this parameter\n",
    "    k = 5\n",
    "    # Get the top K most probable concepts per image\n",
    "    sorted_concepts =  np.argsort(CNN_PREDICTIONS, axis=1)[:,::-1][:,:k]\n",
    "    data_tags = concepts\n",
    "    mlb = MultiLabelBinarizer(classes=range(0,1000))\n",
    "    X_CNN = mlb.fit_transform(sorted_concepts)\n",
    "    #print(tags_bow.shape)\n",
    "    return X_CNN\n",
    "\n",
    "def computeFeatures():\n",
    "    pixels_per_cell = (32, 32)\n",
    "    orientations = 8\n",
    "    \n",
    "    bins = (4,4,4)\n",
    "    hsv = True\n",
    "    \n",
    "    _lemmatize=False\n",
    "    _mdf=3\n",
    "    _metric=\"cosine\"\n",
    "    _k=10\n",
    "    _handles=False\n",
    "    _hashes=False\n",
    "    _case=False\n",
    "    _url=False\n",
    "    \n",
    "    return features_hog(croppedImages, pixels_per_cell, orientations), getColorMatrix(croppedImages, bins, hsv), execute_bow(tweets, _lemmatize, _mdf, _metric, _k, _handles, _hashes, _case, _url), getTagsBow(imageLinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache features\n",
    "X_HOG, X_HOC, X_BOW, X_CNN = computeFeatures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"head2\"></a> Iterative Label Propagation on Web Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a dataset $X=\\{x_1, x_2, \\ldots, x_L, \\ \\ x_{L+1}, \\ldots, x_N\\}$, with $N$ data points, where each $x_i$ consists of some feature representation of document $i$. Given a categories set $C=\\{1, 2, \\ldots, |C|\\}$, it is assumed that the first $L$ data points are labelled with a label $c \\in C$, and the remaining ones are unlabelled.\n",
    "\n",
    "Please refer to the \"Mining Data Graphs\" class (lectured on 29/10), namely slides 28, 29 and 30, for a description of the algorithm steps.\n",
    "\n",
    "For more information, you can check the original paper: Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, Bernhard Schoelkopf. Learning with local and global consistency (2004) http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.115.3219\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a name=\"head21\"></a> Multi-label LP Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the Iterative LP algorithm on your project dataset and discuss its effectiveness.\n",
    "\n",
    "The dataset has a total of 13 categories. The categories of each document are available in the corresponding line of that document (column 'gt_class'), in the provided .csv file. Multiple categories are separated by a comma ','. You should represent each document's categories as a 13-dimensional vector (one-hot encoding), as you did for the MNIST dataset. In this case, you may have more than 1 active dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"head22\"></a> Implement the Multi-label LP Algorithm\n",
    "\n",
    "**Multi-label LP:** As each document has multiple categories, you will need to modify your LP implementation. Instead of an **argmax** to select the final category of each document, you will need to **select the top-k categories**, by applying a threshold on the number of categories assigned.\n",
    "\n",
    "**Discuss:** Discuss examples of thresholds (e.g. select the top-3 labels, keep all categories with their values $>\\alpha$, etc.)  .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAlg(mlb, images, y, y_true, features, weights, selection, topk, threshold, alpha, iterations, params=None, mf = False, indices_unlabeled=[]):\n",
    "\n",
    "    # Step 2 - Normalize Y and Initialize matrix F with Y\n",
    "    Y_hidden = normalize(y, axis = 1, norm=\"l1\")\n",
    "    F = Y_hidden\n",
    "    #print(F[indices_labeled[0],:])\n",
    "    \n",
    "    # Step 3 - Compute matrix W (multi feature -mf true; or single feature -mf false)\n",
    "    W = []\n",
    "    if mf is True:\n",
    "        M = None\n",
    "        for i in range(len(features)):\n",
    "            M = weights[i]*euclidean_distances(features[i], features[i])\n",
    "        #M = weights[0]*euclidean_distances(feature[0], feature[0]) + weights[1]*euclidean_distances(feature[1], feature[1])\n",
    "    else:\n",
    "        M = euclidean_distances(features[0], features[0])\n",
    "        \n",
    "    sigma = np.std(M)\n",
    "    W = np.exp(-1 * M / (2*sigma**2))\n",
    "\n",
    "    # Step 4 - Normalize W\n",
    "    D = np.zeros(W.shape)\n",
    "    np.fill_diagonal(D, W.sum(axis=0))\n",
    "\n",
    "    D12 = np.zeros(D.shape)\n",
    "    from numpy.linalg import inv\n",
    "    D12 = inv(np.sqrt(D))\n",
    "\n",
    "    S = np.dot(D12, W)\n",
    "    S = np.dot(S, D12)\n",
    "\n",
    "    # Step 5 - Perform the F update step num_iterations steps\n",
    "    for i in range(1, iterations):\n",
    "        T1 = alpha * S\n",
    "        T1 = np.dot(T1, F)\n",
    "        T2 =  (1 - alpha) * Y_hidden\n",
    "        F = T1 + T2\n",
    "        #Normalizar para F (verficar segmentos)\n",
    "        F = normalize(F, axis = 1, norm=\"l1\")\n",
    "        \n",
    "    print(\"Indice unlabeled: {}\\nNormalized F: {}\".format(indices_unlabeled[0], F[indices_unlabeled[0],:]))\n",
    "    # Select top k classes\n",
    "    if selection is True:\n",
    "        F = np.fliplr(np.argsort(F, axis=1))\n",
    "        F = F[:,:topk]\n",
    "        Y = mlb.transform(F)\n",
    "    else:\n",
    "        T = []\n",
    "        for row in F:\n",
    "            T.append([i for i, v in enumerate(row) if v >= threshold])\n",
    "        Y = mlb.transform(T)\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runAll(iterations, p, features, weights, alpha, selection, topk, threshold, mf):\n",
    "    #Choose a random number between 1 and 100 to shuffle to prevent biased results\n",
    "    rand_seed = random.randint(1,100)\n",
    "    indices = np.arange(len(tweets))\n",
    "    np.random.seed(rand_seed)\n",
    "    shuffle(indices)\n",
    "\n",
    "    X = tweets\n",
    "    np.random.seed(rand_seed)\n",
    "    shuffle(X)\n",
    "\n",
    "    y_target = targets\n",
    "    np.random.seed(rand_seed)\n",
    "    shuffle(y_target)\n",
    "\n",
    "    total_images = X.shape[0]\n",
    "\n",
    "    # Let's assume that 20% of the dataset is labeled\n",
    "    labeled_set_size = int(total_images*p)\n",
    "\n",
    "    indices_labeled = indices[:labeled_set_size]\n",
    "    indices_unlabeled = indices[labeled_set_size:]\n",
    "    \n",
    "    print(\" \")\n",
    "    print(\"Iteration: {} - Total tweets labeled: {} - Total tweets unlabeled: {}\".format(iterations,\n",
    "        len(indices_labeled), len(indices_unlabeled)))\n",
    "\n",
    "\n",
    "    # Convert labels to a one-hot-encoded vector\n",
    "    # Keep groundtruth labels\n",
    "    classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "    # print(classes)\n",
    "    mlb = MultiLabelBinarizer(classes=classes)\n",
    "    #print(y_target[:1])\n",
    "    Y_true = mlb.fit_transform(y_target)\n",
    "    Y = mlb.transform(y_target)\n",
    "    #print(Y[:1])\n",
    "    # Remove labels of \"unlabeled\" data \n",
    "    Y[indices_unlabeled, :] = np.zeros(Y.shape[1])\n",
    "    \n",
    "    # Run Algorithm and Get Results\n",
    "    Y = runAlg(mlb, croppedImages, Y, Y_true, features=features, weights=weights, selection=selection, topk=topk, threshold=threshold,\n",
    "            alpha=alpha, iterations=iterations, mf=mf, indices_unlabeled=indices_unlabeled)\n",
    "\n",
    "    Y_pred = Y[indices_unlabeled, :]\n",
    "    y_gt = Y_true[indices_unlabeled, :]\n",
    "   \n",
    "    \n",
    "    print(\"Ground Truth: {}\".format(Y_true[indices_unlabeled[0],:]))\n",
    "    print(\"Predicted:    {}\".format(Y[indices_unlabeled[0],:]))\n",
    "    \n",
    "   \n",
    "    return Y_pred, y_gt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"head23\"></a>  Evaluation\n",
    "Evaluate the results of each run of the Iterative LP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "results = np.zeros((10,4))\n",
    "range_iterations = range(10,500,50)\n",
    "j = 0\n",
    "\n",
    "# Variable params\n",
    "p=0.5\n",
    "features=[X_BOW, X_CNN] # Possible X_HOG, X_HOG, X_BOW, X_CNN -> If mf=False use only one feature inside the array\n",
    "weights=[0.5, 0.5] # Sum must be one - and must always be filled even if mf=False\n",
    "alpha=0.8\n",
    "selection=False # False - Top K | True - Threshold\n",
    "topk=3\n",
    "threshold=0.05\n",
    "mf=True\n",
    "\n",
    "for i in range_iterations:\n",
    "    Y_pred, y_gt = runAll(i, p, features, weights, alpha, selection, topk, threshold, mf)\n",
    "    precision, recall, fscore, support = score(y_gt, Y_pred, average='macro')\n",
    "    results[j] = [precision, recall, fscore, support]\n",
    "    j = j+1\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "print(\"\\nResults List:    \\n{}\".format(results))\n",
    "print(\"\\nResults Report:  \\n{}\".format(classification_report(y_gt, Y_pred)))\n",
    "\n",
    "print(\"\\nResults Graph:   \\n\")\n",
    "\n",
    "colors = ['r', 'b', 'g']\n",
    "labels = ['precision', 'recall', 'f-score']\n",
    "\n",
    "for i in range(3):\n",
    "    plt.plot(range_iterations, results[:,i], colors[i], label=labels[i])\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Values\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "#plt.plot(range_iterations, results[:,0], 'r', range_iterations, results[:,1], 'b', range_iterations, results[:,2], 'g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"head24\"></a>  Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which feature spaces are more effective? Do a per-class inspection and understand which feature spaces are more effective for each class and why. \n",
    "\n",
    "# How does the LP algorithm behaves when you change the number of initial labels? (variable labeled_set_size on lab4)\n",
    "\n",
    "# Note that documents (Tweets) from your project's dataset have multiple labels, i.e. each document may belong to 1 or more classes. \n",
    "# Discuss how this impacts the label propagation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a name=\"head3\"></a> Multi-Feature Iterative Label Propagation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the computation of the affinity matrix S.\n",
    "\n",
    "Given some feature space representation, each entry $w_{ij}$, for $i\\neq j$ is computed as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w_{ij} = exp\\Big({-\\frac{||x_i - x_j||^2}{2\\sigma^2}}\\Big),\n",
    "\\end{align}\n",
    "$$\n",
    "where a Gaussian kernel is applied over the distance on the considered feature space.\n",
    "\n",
    "\n",
    "In order compute affinity by considering **multiple feature spaces**, the above expression can be extended as:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "w_{ij} = exp\\Big({-\\frac{\\Big[\\sum_{f}\\alpha_f\\cdot||x_i^f - x_j^f||\\Big]^2}{2\\sigma^2}}\\Big),\n",
    "\\end{align}\n",
    "$$\n",
    "where each $x^f$ denotes a given feature space and $\\alpha_f$ the weight associated with that space. The weights should be defined such that $\\sum_f \\alpha_f = 1$.\n",
    "\n",
    "You can define the contribution of each feature space by adequately setting its associated weight $\\alpha_f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <a name=\"head31\"></a> Implement the Multi-Feature Iterative Label Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that given the Multi-label implementation of Iterative LP, you should only need to change the computation of each $w_{ij}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a name=\"head32\"></a>  Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discuss the effectiveness of the Multi-Feature approach versus the Single feature variant. Namely, compare HoC+HoG with VGG.\n",
    "\n",
    "# Change the weights of each feature space and interpret the results. Which features better contribute to the overall effectiveness?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
